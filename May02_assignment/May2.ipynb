{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9424157f-6830-449b-bd3b-980feac20802",
   "metadata": {},
   "source": [
    "# Q1. What is anomaly detection and what is its purpose?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53875007-cd1b-4257-8aa7-0d105b607cc6",
   "metadata": {},
   "source": [
    "Anomaly detection is a technique used in data analysis to identify unusual patterns or observations that do not conform to the expected behavior of a given dataset. The purpose of anomaly detection is to find data points that are significantly different from the majority of the data points in a dataset.\n",
    "\n",
    "Anomalies can be caused by various factors such as measurement errors, data corruption, or unexpected events that lead to a deviation from the normal pattern. Anomaly detection techniques can be applied to a wide range of domains, such as fraud detection, intrusion detection, medical diagnosis, and quality control, among others.\n",
    "\n",
    "The goal of anomaly detection is to identify these unusual data points, investigate the cause of these anomalies, and take appropriate actions to address them. By detecting anomalies early, businesses can prevent potential risks, reduce costs, improve product quality, and enhance overall operational efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d63304-93d2-4ce7-82b5-75467a394ac7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "400fb869-25dd-49c3-8e36-f28a40107453",
   "metadata": {},
   "source": [
    "# Q2. What are the key challenges in anomaly detection?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f30f4e5-9a2c-438c-b6f2-1b59405430be",
   "metadata": {},
   "source": [
    "### The key challenges in anomaly detection:\n",
    "\n",
    "1. `Lack of labeled data`: It's difficult to obtain labeled data for anomalies since they are rare occurrences, making it hard to train and evaluate anomaly detection models.\n",
    "\n",
    "2. `Imbalanced datasets`: Anomaly detection datasets are often imbalanced, with a majority of normal instances and a few anomalies. This can affect the performance of models, which may struggle to detect anomalies accurately.\n",
    "\n",
    "3. `Adaptability to evolving anomalies`: Anomaly detection models need to adapt to changing data patterns and detect new types of anomalies that were not present during training.\n",
    "\n",
    "4. `Determining appropriate thresholds`: Setting thresholds to differentiate normal and abnormal instances is challenging, as it requires balancing false positives and false negatives.\n",
    "\n",
    "5. `High-dimensional data`: Real-world datasets are often high-dimensional, posing computational challenges and requiring techniques like feature selection or dimensionality reduction.\n",
    "\n",
    "6. `Interpretability`: Anomaly detection models should provide explanations for their detected anomalies, but it can be challenging for complex models to provide understandable justifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc36a0e5-801d-4c69-b5a6-ebc94f171916",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d355495a-6105-4bec-8e64-4cda4d22778e",
   "metadata": {},
   "source": [
    "# Q3. How does unsupervised anomaly detection differ from supervised anomaly detection?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c327d0-3c5b-4f47-870e-1640e4643987",
   "metadata": {},
   "source": [
    "1. In supervised anomaly detection, the training dataset is labeled, meaning that each data point is labeled as either normal or anomalous. This labeled data is used to train a model to recognize patterns and characteristics of normal data. In contrast, unsupervised anomaly detection does not rely on labeled data. It assumes that the majority of the data is normal and aims to identify anomalies based on deviations from the normal patterns within the dataset.\n",
    "\n",
    "2. Supervised anomaly detection requires pre-labeled data with information about normal and anomalous instances. These labels are used to train a model to distinguish between normal and anomalous data. Unsupervised anomaly detection does not require any labeled data and can work with unlabeled datasets, making it more suitable when labeled anomalies are scarce or not available.\n",
    "\n",
    "The key difference between the two approaches is the availability of labeled data. Supervised methods require labeled data, which may not always be available, while unsupervised methods do not require labeled data and are more appropriate for scenarios where labeled data is scarce or unavailable. However, supervised methods tend to be more accurate than unsupervised methods since they can learn from labeled data and provide more reliable classifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91dae9ca-2e92-4fec-886b-09467b1715ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5542d1a3-5a04-444e-8bd3-d090f5fa6657",
   "metadata": {},
   "source": [
    "# Q4. What are the main categories of anomaly detection algorithms?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d3cba8-e389-4eb7-b5aa-ada163031077",
   "metadata": {},
   "source": [
    "The main categories of anomaly detection algorithms include:\n",
    "\n",
    "0. `Distance-Based Methods`: Distance-based methods measure the similarity or dissimilarity between data points and use distance metrics to identify anomalies. Instances that are significantly different or distant from others are considered anomalies. Distance-based techniques include k-nearest neighbors (k-NN) and Local Outlier Factor (LOF).\n",
    "\n",
    "1. `Statistical-based methods`: These methods rely on statistical models to identify data points that deviate significantly from the expected behavior. Examples of statistical-based methods include Gaussian Mixture Models (GMM), Principal Component Analysis (PCA), and Time-series Analysis.\n",
    "\n",
    "2. `Machine learning-based methods`: These methods use machine learning algorithms to learn the patterns of normal behavior in the data and detect deviations from these patterns. Examples of machine learning-based methods include Support Vector Machines (SVM), Decision Trees, Random Forests, and Neural Networks.\n",
    "\n",
    "3. `Clustering-based methods`: These methods group similar data points together and identify data points that do not belong to any cluster as anomalies. Examples of clustering-based methods include K-means, DBSCAN, and Hierarchical Clustering.\n",
    "\n",
    "4. `Density-based methods`: These methods identify anomalies based on deviations from the density of the data. Examples of density-based methods include Local Outlier Factor (LOF), Isolation Forests, and Kernel Density Estimation.\n",
    "\n",
    "5. `Rule-based methods`: These methods define rules or thresholds that identify data points that fall outside of the expected range. Examples of rule-based methods include statistical process control and expert systems.\n",
    "\n",
    "Each category of anomaly detection methods has its strengths and weaknesses, and the appropriate method depends on the type of data, the context of the problem, and the specific requirements of the application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff5d52c-7c50-42e1-8e9d-af8b820ad0ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "914f7db9-0067-44d2-9d6a-20a7f3effbb4",
   "metadata": {},
   "source": [
    "# Q5. What are the main assumptions made by distance-based anomaly detection methods?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9aec4f0-53b6-4ce2-a7b2-78b6ceab51ef",
   "metadata": {},
   "source": [
    "Distance-based anomaly detection methods rely on the assumption that normal data points are clustered tightly together, while anomalies are far away from the normal cluster. The main assumptions made by distance-based anomaly detection methods are:\n",
    "\n",
    "1. `Normal data points are close to each other`: Distance-based methods assume that normal data points are similar to each other and form a tight cluster, while anomalies are far from the cluster. This assumption is based on the intuition that normal behavior is repetitive and predictable.\n",
    "\n",
    "2. `Anomalies are isolated`: Distance-based methods assume that anomalies are isolated data points that are far from the normal cluster. This assumption is based on the idea that anomalies are rare and occur infrequently, making them outliers in the data.\n",
    "\n",
    "3. `Distance metric is appropriate`: Distance-based methods rely on a distance metric to measure the similarity between data points. The assumption is that the distance metric used is appropriate for the data and can accurately capture the similarity between data points.\n",
    "\n",
    "4. `Data is numerical`: Distance-based methods assume that the data is numerical and can be represented as points in a high-dimensional space. This assumption makes it challenging to apply distance-based methods to categorical or text data.\n",
    "\n",
    "It is essential to validate these assumptions before applying distance-based anomaly detection methods to a specific dataset. If the assumptions are not met, the results of the analysis may be unreliable, and other anomaly detection methods may be more appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e5015a-7957-4e06-a783-81593d6e6cdb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c25732aa-37ae-45e6-a0f9-6e231446bb15",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Q6. How does the LOF algorithm compute anomaly scores?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb3a360-8c40-4c27-a692-514fecc7c7ba",
   "metadata": {},
   "source": [
    "The Local Outlier Factor (LOF) algorithm computes anomaly scores by measuring the local density of a data point with respect to its neighbors. The LOF algorithm identifies data points that have a significantly lower density than their neighbors as anomalies.\n",
    "\n",
    "The algorithm works as follows:\n",
    "\n",
    "1. For each data point, identify its k nearest neighbors based on a distance metric (e.g., Euclidean distance). The value of k is a user-defined parameter.\n",
    "\n",
    "2. Compute the local reachability density (LRD) of the data point as the inverse of the average distance between the data point and its k nearest neighbors.\n",
    "\n",
    "3. Compute the local outlier factor (LOF) of the data point as the average LRD of its k nearest neighbors divided by its own LRD.\n",
    "\n",
    "4. Anomalies are identified as data points with an LOF score that is significantly higher than the LOF scores of their neighbors. The threshold for identifying anomalies is also a user-defined parameter.\n",
    "\n",
    "Intuitively, the LOF algorithm computes the density of a data point in relation to its neighbors and identifies data points that are significantly less dense than their neighbors as anomalies. This approach can detect anomalies that are not isolated, but rather exist in low-density regions of the data.\n",
    "\n",
    "The LOF algorithm is widely used in anomaly detection applications and is especially useful for datasets with non-uniform density distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d86263-9189-4c4a-b6cb-73451fd2afb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a33ee977-2ed2-4fb2-9684-6de39112f395",
   "metadata": {},
   "source": [
    "# Q7. What are the key parameters of the Isolation Forest algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51458514-508f-4109-995d-5c70256a92cd",
   "metadata": {},
   "source": [
    "The Isolation Forest algorithm, a popular anomaly detection algorithm, has a few key parameters that can be adjusted to influence its performance. Here are the main parameters of the Isolation Forest algorithm:\n",
    "\n",
    "1. `n_estimators`: This parameter determines the number of isolation trees to be created. An isolation tree is a sub-sample of the original data that is randomly generated. Increasing the number of trees can improve the performance of the algorithm but may also increase computation time.\n",
    "\n",
    "2. `max_samples`: It defines the number of samples to be used for building each isolation tree. Higher values can lead to more reliable outlier detection, but they can also increase computational overhead.\n",
    "\n",
    "3. `max_features`: This parameter controls the number of features randomly selected for splitting each tree node. It can influence the diversity of trees and the ability to capture different aspects of the data. The default value is often set to the square root of the total number of features.\n",
    "\n",
    "4. `contamination`: This parameter specifies the expected proportion of anomalies in the dataset. It helps in setting a threshold for deciding what fraction of instances should be classified as anomalies. If the actual proportion of anomalies is known, it can be directly provided. Otherwise, a rough estimate can be used.\n",
    "\n",
    "5. `random_state`: This parameter is used to initialize the random number generator. Providing a fixed value for random_state ensures reproducibility of results when the algorithm is run multiple times with the same parameters and dataset.\n",
    "\n",
    "Tuning these parameters can impact the performance of the Isolation Forest algorithm. It is recommended to experiment with different values and evaluate the results using appropriate evaluation metrics or domain-specific requirements to find the optimal parameter configuration for a given application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06e58be-9979-471d-91fd-997454d0b9f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e60c84bb-eaeb-45a7-ae7b-1961bc65dd65",
   "metadata": {},
   "source": [
    "# Q8. If a data point has only 2 neighbours of the same class within a radius of 0.5, what is its anomaly score using KNN with K=10?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f74f124-d44c-4f4f-b0f1-e8134aac4c2f",
   "metadata": {},
   "source": [
    "To compute the anomaly score of a data point using the KNN (K-Nearest Neighbors) algorithm with K=10, we need to consider the majority class of its K nearest neighbors. In your scenario, if the data point has only 2 neighbors of the same class within a radius of 0.5, it means that it has 2 neighbors belonging to the same class and no neighbors of the other class within that radius.\n",
    "\n",
    "Since K=10, and only 2 out of the 10 nearest neighbors are of the same class, we can expect that the remaining 8 neighbors belong to the other class. Based on this, we can infer that the data point is an outlier or an anomaly, as it has a significantly lower number of neighbors of the same class compared to the majority of its neighbors.\n",
    "\n",
    "Therefore, the anomaly score of this data point using KNN with K=10 would be relatively high, indicating its deviation from the majority class in its local neighborhood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318febdd-dfc5-4161-bfca-3b810dfcacf5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6da74701-b9ec-48e1-9ec9-c78e4df833be",
   "metadata": {},
   "source": [
    "# Q9. Using the Isolation Forest algorithm with 100 trees and a dataset of 3000 data points, what is the anomaly score for a data point that has an average path length of 5.0 compared to the average path length of the trees?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c146de96-4921-4043-839f-3528e646652f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d40a49de-f02d-4ebc-aa34-694aff1d9cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.rand(3000,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7859b133-9bd6-4894-bbad-740f0c89cc90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.09532581, 0.40857426, 0.62177116, ..., 0.16578132, 0.993635  ,\n",
       "        0.29874057],\n",
       "       [0.07117114, 0.48485285, 0.66100469, ..., 0.40840642, 0.75261472,\n",
       "        0.07015746],\n",
       "       [0.61021083, 0.63429304, 0.17128535, ..., 0.87471076, 0.0128079 ,\n",
       "        0.99050169],\n",
       "       ...,\n",
       "       [0.86267046, 0.14109599, 0.89658373, ..., 0.92059917, 0.96529678,\n",
       "        0.78802216],\n",
       "       [0.07357098, 0.25838052, 0.48236518, ..., 0.85725373, 0.67022951,\n",
       "        0.16760705],\n",
       "       [0.10005414, 0.63911595, 0.14102439, ..., 0.07952977, 0.45846361,\n",
       "        0.48266322]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f296d9a8-bfbe-4a81-974e-14d64136910e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "71cdf569-e25b-4a2a-9648-fe2c36953a7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>IsolationForest(random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">IsolationForest</label><div class=\"sk-toggleable__content\"><pre>IsolationForest(random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "IsolationForest(random_state=42)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = IsolationForest(n_estimators=100,contamination='auto',random_state=42)\n",
    "clf.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc638596-16ae-4308-858d-fd42bcdd85d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = clf.fit_predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7f9ed0f5-1367-4928-817e-d41e0a668212",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1, -1, -1, ..., -1, -1, -1])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b719c30c-8715-464b-b363-1e1ba680b62c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.53450568 -0.51290801 -0.54938894 ... -0.52651302 -0.50554336\n",
      " -0.53227795]\n"
     ]
    }
   ],
   "source": [
    "anomaly_scores = clf.score_samples(X)\n",
    "\n",
    "# Print the anomaly scores\n",
    "print(anomaly_scores)\n",
    "\n",
    "\n",
    "# Compute the mean of the anomaly scores\n",
    "mean_anomaly_score = np.mean(anomaly_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bcaaf056-6ee2-44ab-910b-6aac9dcc3324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The mean anomaly score is -0.5128\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nThe mean anomaly score is {mean_anomaly_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af2164a-b6c1-46e7-b37c-30565bfca870",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
